---
layout: page
---

<h2>APCs</h2>

<h3>Predictive Comparisons Generalize Regression Coefficients</h3>

<p>At the heart of this package is the idea of a <em>predictive comparison</em>: We vary the input of interest holding the other inputs constant, and look at the differences in predicted values. Let \(u\) represent the input of interest and \(v\) the (vector of) other inputs. Let \(f\) be a functinon making predictions, so</p>

<p>\[\hat{y} = f(u,v)\]</p>

<p>Our \(f\) could come from any predictive model. If we have a statistical model, then probably we would choose </p>

<p>\[f(u,v) = \mathcal{E}[y \mid u, v, \theta]\]</p>

<p>(where \(\theta\) are the parameters of the model). But we need not have a statistical model at all. The prediction function \(f\) could come from a random forest, or a support vector machine.</p>

<p>Given the function \(f\) and a choice of \(u_1\), \(u_2\), and \(v\), we can compute</p>

<p>\[\delta_{u_1 \rightarrow u_2, v} = \frac{f(u_2, v) - f(u_1, v)}{u_2-u_1}\]</p>

<p>If \(f\) were a linear model with no interactions, the above would not depend on the particular choices of \(u_1\), \(u_2\), and \(v\) and would be the regression coefficient corresponding to \(u\). This is the formal sense in which predictive comparisons generalize regression coefficients. Since for more complicated models this varies as the inputs vary, we will take an average across a well chosen set of inputs. </p>

<h3>Choice of Inputs</h3>

<p>The APC is</p>

<p>\[\frac{\mathcal{E}[\Delta_f]}{\mathcal{E}[\Delta_u]}\]</p>

<p>where \(\Delta_f = f(u_2,v) - f(u_1,v)\), \(\Delta_u = u_2 - u_1\), and \(\mathcal{E}\) is expectation under the following process:</p>

<ol>
<li>sample \(v\) from the (marginal) distribution of the corresponding inputs</li>
<li>sample \(u_1\) and \(u_2\) independently from the distribution of \(u\) conditional on \(v\)</li>
</ol>

<p>The reason for this definition is that we want to use values of \(v\) that are representative of our data generation process, and transitions in \(u\) that are representative of what really occurs at those values of \(v\).</p>

<p>Computing the numerator and denominator separately rather than taking an expected value of \(\delta_{u_1 \rightarrow u_2, v}\) amounts to weighting by the size of \((u_2 - u_1)\). This avoids having the result excessively influenced by small changes in \(u\).</p>

<h3>Estimation</h3>

<p>The rows of our data represent samples from the joint distribution \(u\), \(v\), so each row amounts to a sample from \(v\) followed by a sample \(u_1\) conditional on \(v\). The difficult thing is drawing another sample \(u_2\) conditional on \(v\). We approximate these by assigning weights to rows based on the proximity of the \(v\) in that row to the \(v\) in the row in question.</p>

<p>For more details, see <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9531.2007.00181.x/abstract">Gelman and Pardoe 2007</a> (section 4), <a href="more-renormalize-weights.html">my note</a> explaining a small change from the weights described in the paper, or <a href="https://github.com/dchudz/predcomps/blob/master/R/pairs.R">my code</a> that computes the appropriate weights.</p>

<h3>A Small Example</h3>

<p>(logistic regression example with correlation and no interaction)</p>


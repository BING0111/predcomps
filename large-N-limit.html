---
layout: page
---

<h2>Large \(N\) Limit</h2>

<p>As we get more data, it would be nice if the APC we compute tends toward the right answer, equation (2) of <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9531.2007.00181.x/abstract">Gelman and Pardoe 2007</a>. I don&#39;t care about asymptotics as much as some people, but if we don&#39;t get the right answer in the limit, that&#39;s at least a clue that we might not be doing as well as we can for smaller \(N\). This note shows that unless we adjust the weighting function as we get more data, we won&#39;t have that nice property.</p>

<pre><code class="r">makeExampleDF &lt;- function(N) {
  exampleDF &lt;- data.frame(
    v=c(3,3,7,7),  
    u=c(10,20,12,22) 
    )[rep(c(1,2,3,4),c(.4*N,.4*N,.1*N,.1*N)),]
  exampleDF &lt;- transform(exampleDF, v = v + rnorm(nrow(exampleDF), sd=.001))
  return(exampleDF)
}
</code></pre>

<p>Just as in the note &ldquo;Normalizing Weights&rdquo;, the APC should be:</p>

<p>\[.8 \delta_u(10 \rightarrow 20, 3, f) + 0.2 \delta_u(12 \rightarrow 22, 7, f) = (.8)(3) + (.2)(6) = 3.8\]</p>

<p>We get almost the same APC with 300 data points as 100:</p>

<pre><code class="r">GetAPC(function(df) return(df$u * df$v), makeExampleDF(100), u=&quot;u&quot;, v=&quot;v&quot;)
</code></pre>

<pre><code>## [1] 3.855
</code></pre>

<pre><code class="r">GetAPC(function(df) return(df$u * df$v), makeExampleDF(300), u=&quot;u&quot;, v=&quot;v&quot;)
</code></pre>

<pre><code>## [1] 3.845
</code></pre>

<p>If we&#39;re looking at one value for \(v\), \(v=v_0\), the tradeoff in determining the weights is:</p>

<ol>
<li>\(v\)&#39;s closer to \(v_0\) will do a better job representing the distribution of \(u\) conditional on \(v=v_0\)</li>
<li>but if too few \(v\)&#39;s get too much of the weight, our estimate for the conditional distribution of \(u\) will be too noisy</li>
</ol>

<p>The reason our estimate didn&#39;t improve with more data is that we&#39;re not moving more weight to nearby points as we get more data. For any \(N\), we&#39;re presently putting roughly the same amount of mass at the same distances. As we get more data, we can afford to put more weight closer to \(v\), because (2) becomes less of a problem. A couple ideas are:</p>

<ul>
<li>With the weights as \(\frac{1}{k+d}\) (\(d\) is the Mahalanobis distance), we could scale \(k\) down as \(N\) goes up.</li>
<li>Or we could use the weights we are now (\(k=1\)), except we drop (equivalent to setting the weight to 0) all but the closest \(s(N)\) points to each \(v\). The function \(s\) needs to increases with \(N\), but not as fast as \(N\), e.g. maybe \(s(N) = sqrt(N)\) probably works. This means we&#39;re always decreasing bias (sampling from closer to the right \(v\)) and also decreasing variance (more samples) as N increases. This would also be good for keeping run-times and memory usage under control as \(N\) increases.</li>
</ul>

